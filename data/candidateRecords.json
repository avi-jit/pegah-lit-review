{"papers":[{"url":"https://www.semanticscholar.org/paper/b13c3e87a80f491899068524e7e860872b521a27","title":"DIONYSUS: A Pre-trained Model for Low-Resource Dialogue Summarization","venue":"ArXiv","year":2022,"referenceCount":56,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Yu Li,Baolin Peng,Pengcheng He,Michel Galley,Zhou Yu,Jianfeng Gao","id":"b13c3e87a80f491899068524e7e860872b521a27","summary":"DIONYSUS (dynamic input optimization in pre-training for dialogue summarization), a pre-trained encoder-decoder model for summarizing dialogues in any new domain outperforms existing methods on six datasets, as demonstrated by its ROUGE scores in zero-shot and few-shot settings.","score":2},{"url":"https://www.semanticscholar.org/paper/3e50d38ffc6cc01d4adeda5f35dbfdef2cc91dc6","title":"SemEval-2020 Task 4: Commonsense Validation and Explanation","venue":"International Workshop on Semantic Evaluation","year":2020,"referenceCount":69,"citationCount":63,"influentialCitationCount":11,"publicationDate":"01/07/2020","authors":"Cunxiang Wang,Shuailong Liang,Yili Jin,Yilong Wang,Xiaodan Zhu,Yue Zhang","id":"3e50d38ffc6cc01d4adeda5f35dbfdef2cc91dc6","summary":"This paper presents SemEval-2020 Task 4,CommonsenseValidation andExplanation (ComVE), which includes three subtasks, aiming to evaluate whether a system can distinguish anatural language statement that makes senseto humans from one that does not, and provide thereasons.","score":1},{"url":"https://www.semanticscholar.org/paper/70f2c1567ef94fdf4581e1290bf7667cc9a4dcfc","title":"LogiQA: A Challenge Dataset for Machine Reading Comprehension with Logical Reasoning","venue":"International Joint Conference on Artificial Intelligence","year":2020,"referenceCount":40,"citationCount":56,"influentialCitationCount":17,"publicationDate":"09/07/2020","authors":"Jian Liu,Leyang Cui,Hanmeng Liu,Dandan Huang,Yile Wang,Yue Zhang","id":"70f2c1567ef94fdf4581e1290bf7667cc9a4dcfc","summary":"A comprehensive dataset, named LogiQA, is built, which is sourced from expert-written questions for testing human Logical reasoning, and shows that state-of-the-art neural models perform by far worse than human ceiling.","score":1},{"url":"https://www.semanticscholar.org/paper/1cad933afc55f1a562e27ebd4f65c5d0f5a6c26a","title":"Task-specific Objectives of Pre-trained Language Models for Dialogue Adaptation","venue":"ArXiv","year":2020,"referenceCount":47,"citationCount":17,"influentialCitationCount":1,"publicationDate":"10/09/2020","authors":"Junlong Li,Zhuosheng Zhang,Hai Zhao,Xi Zhou,Xiang Zhou","id":"1cad933afc55f1a562e27ebd4f65c5d0f5a6c26a","summary":"A Dialogue-Adaptive Pre-training Objective (DAPO) based on some important qualities for assessing dialogues which are usually ignored by general LM pre-training objectives is designed and Experimental results show that models with DAPO surpass those with general LMPre- training objectives and other strong baselines on downstream DrNLP tasks.","score":1},{"url":"https://www.semanticscholar.org/paper/d8ea988072efb115ee8c85e159c1fa4a816360b5","title":"Does BERT Solve Commonsense Task via Commonsense Knowledge?","venue":"ArXiv","year":2020,"referenceCount":45,"citationCount":19,"influentialCitationCount":1,"publicationDate":"10/08/2020","authors":"Leyang Cui,Sijie Cheng,Yu Wu,Yue Zhang","id":"d8ea988072efb115ee8c85e159c1fa4a816360b5","summary":"This work proposes two attention-based methods to analyze commonsense knowledge inside BERT, and finds that attention heads successfully capture the structured commonsenseknowledge encoded in ConceptNet, which helps BERT solve commonsense tasks directly.","score":1},{"url":"https://www.semanticscholar.org/paper/6e323af2a690128783a3f04df4eabcccb437dad5","title":"An Empirical Study on Deep Neural Network Models for Chinese Dialogue Generation","venue":"Symmetry","year":2020,"referenceCount":48,"citationCount":1,"influentialCitationCount":0,"publicationDate":"23/10/2020","authors":"Zhe Li,M. Maimaiti,Jiabao Sheng,Zunwang Ke,Wushour Silamu,Qinyong Wang,Xiuhong Li","id":"6e323af2a690128783a3f04df4eabcccb437dad5","summary":"An empirical study for state-of-the-art DNN-based dialogue generation models in various Chinese corpora to evaluate a wide range of dialoguegeneration models that are based on the symmetrical architecture of Seq2Seq, RNNSearch, transformer, generative adversarial nets, and reinforcement learning respectively.","score":1},{"url":"https://www.semanticscholar.org/paper/1e4704c54059fa1e5d2d0ec40d5ebaa784f290f7","title":"Generating Math Word Problems from Equations with Topic Controlling and Commonsense Enforcement","venue":"ArXiv","year":2020,"referenceCount":51,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/12/2020","authors":"Tianyang Cao,Shuang Zeng,Songge Zhao,Mairgup Mansur,Baobao Chang","id":"1e4704c54059fa1e5d2d0ec40d5ebaa784f290f7","summary":"This paper presents a novel equation-to-problem text generation model, which outperforms baseline and previous models in both accuracy and richness of generated problem text.","score":1},{"url":"https://www.semanticscholar.org/paper/0e2d653a6a2961fe9cbc2b99e5380975ed550633","title":"Generating Math Word Problems from Equations with Topic Consistency Maintaining and Commonsense Enforcement","venue":"International Conference on Artificial Neural Networks","year":2021,"referenceCount":4,"citationCount":2,"influentialCitationCount":1,"publicationDate":2021,"authors":"Tianyang Cao,Shuang Zeng,Songge Zhao,Mairgup Mansur,Baobao Chang","id":"0e2d653a6a2961fe9cbc2b99e5380975ed550633","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/dfcdf6c4e1d43097d911e0da304b2540f857cfb8","title":"DialogSum Challenge: Summarizing Real-Life Scenario Dialogues","venue":"International Conference on Natural Language Generation","year":2021,"referenceCount":22,"citationCount":14,"influentialCitationCount":2,"publicationDate":2021,"authors":"Yulong Chen,Yang Liu,Yue Zhang","id":"dfcdf6c4e1d43097d911e0da304b2540f857cfb8","summary":"This work carefully annotates a large-scale dialogue summarization dataset based on multiple public dialogue corpus, opening the door to all kinds of summarization models.","score":1},{"url":"https://www.semanticscholar.org/paper/51be27f16fcfa68fb2c91a63058ec4e43cef8eb3","title":"CTRD: A Chinese Theme-Rheme Discourse Dataset","venue":"Natural Language Processing and Chinese Computing","year":2021,"referenceCount":14,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"Biao Fu,Yiqi Tong,Dawei Tian,Yidong Chen,X. Shi,Ming Zhu","id":"51be27f16fcfa68fb2c91a63058ec4e43cef8eb3","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/562236dbb6651370d8ab0c0a194773d032de76c8","title":"Graphs and Commonsense Knowledge improve the Dialogue Reasoning Ability","venue":"International Workshop on the Semantic Web","year":2021,"referenceCount":5,"citationCount":0,"influentialCitationCount":0,"publicationDate":2021,"authors":"Minglei Gao,Sai Zhang,Xiaowang Zhang,Zhiyong Feng,Wenhuan Lu","id":"562236dbb6651370d8ab0c0a194773d032de76c8","summary":"This paper is using the advantages of graph structure in reasoning, putting the context and candidate responses in the same graph, and using commonsense knowledge to explicitly show the associated features, thereby improving the dialogue system’s reasoning ability.","score":1},{"url":"https://www.semanticscholar.org/paper/2cc805b3b4a0a6a619a44bb7dd6d91d15f117016","title":"Think Before You Speak: Learning to Generate Implicit Knowledge for Response Generation by Self-Talk","venue":"NLP4CONVAI","year":2021,"referenceCount":17,"citationCount":4,"influentialCitationCount":1,"publicationDate":2021,"authors":"Pei Zhou,Behnam Hedayatnia,Karthik Gopalakrishnan,Seokhwan Kim,J. Pujara,Xiang Ren,Yang Liu,Dilek Z. Hakkani-Tür","id":"2cc805b3b4a0a6a619a44bb7dd6d91d15f117016","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/0934d7cac5a86b02fc49852334051bde540b34bd","title":"DialogSum: A Real-Life Scenario Dialogue Summarization Dataset","venue":"Findings","year":2021,"referenceCount":37,"citationCount":55,"influentialCitationCount":19,"publicationDate":2021,"authors":"Yulong Chen,Yang Liu,Liang Chen,Yue Zhang","id":"0934d7cac5a86b02fc49852334051bde540b34bd","summary":"Experimental results show unique challenges in dialogue summarization, such as spoken terms, special discourse structures, coreferences and ellipsis, pragmatics and social common sense, which require specific representation learning technologies to better deal with.","score":1},{"url":"https://www.semanticscholar.org/paper/1db86e01300e2f30fd08b46e63ea11656cb6dcf5","title":"TGEA: An Error-Annotated Dataset and Benchmark Tasks for TextGeneration from Pretrained Language Models","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":57,"citationCount":2,"influentialCitationCount":1,"publicationDate":2021,"authors":"Jie He,Bo Peng,Yi Liao,Qun Liu,Deyi Xiong","id":"1db86e01300e2f30fd08b46e63ea11656cb6dcf5","summary":"TGEA, an error-annotated dataset with multiple benchmark tasks for text generation from pretrained language models (PLMs), is proposed, which is the first dataset with comprehensive annotations for PLM-generated texts, which facilitates the diagnostic evaluation ofPLM-based text generation.","score":1},{"url":"https://www.semanticscholar.org/paper/c3d4f9f721a2b1164f043d7ca2db10daaeb19e68","title":"Do It Once: An Embarrassingly Simple Joint Matching Approach to Response Selection","venue":"Findings","year":2021,"referenceCount":15,"citationCount":1,"influentialCitationCount":0,"publicationDate":2021,"authors":"Linhao Zhang,Dehong Ma,Sujian Li,Houfeng Wang","id":"c3d4f9f721a2b1164f043d7ca2db10daaeb19e68","summary":"A joint matching (JM) approach which performs matching only once regardless of the number of options is explored, which enables a cheap but effective data augmentation method.","score":1},{"url":"https://www.semanticscholar.org/paper/e764dee4e50db01d77976e8f313fc092fc0eba85","title":"GRICE: A Grammar-based Dataset for Recovering Implicature and Conversational rEasoning","venue":"Findings","year":2021,"referenceCount":70,"citationCount":1,"influentialCitationCount":0,"publicationDate":2021,"authors":"Zilong Zheng,Shuwen Qiu,Lifeng Fan,Yixin Zhu,Song-Chun Zhu","id":"e764dee4e50db01d77976e8f313fc092fc0eba85","summary":"A grammar-based dialogue dataset, GRICE, designed to bring implicature into pragmatic reasoning in the context of conversations, and shows an overall performance boost in conversational reasoning.","score":1},{"url":"https://www.semanticscholar.org/paper/011869f932f89d047ce2bd36d73a95cc04888193","title":"RICA: Evaluating Robust Inference Capabilities Based on Commonsense Axioms","venue":"Conference on Empirical Methods in Natural Language Processing","year":2020,"referenceCount":55,"citationCount":23,"influentialCitationCount":1,"publicationDate":"02/05/2020","authors":"Pei Zhou,Rahul Khanna,Bill Yuchen Lin,Daniel Ho,J. Pujara,Xiang Ren","id":"011869f932f89d047ce2bd36d73a95cc04888193","summary":"A new challenge, RICA: Robust Inference using Commonsense Axioms, that evaluates robust commonsense inference despite textual perturbations and shows that PTLMs perform no better than random guessing on the zero-shot setting, are heavily impacted by statistical biases, and are not robust to perturbation attacks.","score":1},{"url":"https://www.semanticscholar.org/paper/07b95736960731b49b6ce5aa0b29f10bd0586a6d","title":"On Commonsense Cues in BERT for Solving Commonsense Tasks","venue":"Findings","year":2020,"referenceCount":55,"citationCount":6,"influentialCitationCount":0,"publicationDate":"10/08/2020","authors":"Leyang Cui,Sijie Cheng,Yu Wu,Yue Zhang","id":"07b95736960731b49b6ce5aa0b29f10bd0586a6d","summary":"Using two different measures, it is found that BERT does use relevant knowledge for solving the task, and the presence of commonsense knowledge is positively correlated to the model accuracy.","score":1},{"url":"https://www.semanticscholar.org/paper/8f088ede342f2aaaf6de553f4eb741f1585c60c3","title":"Filling the Gap of Utterance-aware and Speaker-aware Representation for Multi-turn Dialogue","venue":"AAAI Conference on Artificial Intelligence","year":2020,"referenceCount":54,"citationCount":18,"influentialCitationCount":2,"publicationDate":"14/09/2020","authors":"Longxiang Liu,Zhuosheng Zhang,Hai Zhao,Xi Zhou,Xiang Zhou","id":"8f088ede342f2aaaf6de553f4eb741f1585c60c3","summary":"A novel model is proposed to fill the gap in retrieval-based multi-turn dialogue modeling by modeling the effective utterance-aware and speaker-aware representations entailed in a dialogue history by decouple the contextualized word representations by masking mechanisms in Transformer-based PrLM.","score":1},{"url":"https://www.semanticscholar.org/paper/ffbfce72f12aa0be619be5e49698c2657853409f","title":"Natural Language Inference in Context - Investigating Contextual Reasoning over Long Texts","venue":"AAAI Conference on Artificial Intelligence","year":2020,"referenceCount":52,"citationCount":13,"influentialCitationCount":2,"publicationDate":"10/11/2020","authors":"Hanmeng Liu,Leyang Cui,Jian Liu,Yue Zhang","id":"ffbfce72f12aa0be619be5e49698c2657853409f","summary":"ConTRoL is a new dataset for ConTextual Reasoning over Long texts, a passage-level NLI dataset with a focus on complex contextual reasoning types such as logical reasoning, derived from competitive selection and recruitment test for police recruitment with expert level quality.","score":1},{"url":"https://www.semanticscholar.org/paper/9698cff93ec15e4c92b1fccb2332673ef4074899","title":"A Graph Reasoning Network for Multi-turn Response Selection via Customized Pre-training","venue":"AAAI Conference on Artificial Intelligence","year":2020,"referenceCount":33,"citationCount":9,"influentialCitationCount":2,"publicationDate":"21/12/2020","authors":"Yongkang Liu,Shi Feng,Daling Wang,Kaisong Song,Feiliang Ren,Yifei Zhang","id":"9698cff93ec15e4c92b1fccb2332673ef4074899","summary":"A graph- reasoning network (GRN) is proposed to address response selection for multi-turn conversation in retrieval-based chatbots and can dramatically outperform the strong baseline methods and can achieve performance which is close to human-level.","score":1},{"url":"https://www.semanticscholar.org/paper/114be5db62209a0d0682279f5a054a316f56697e","title":"Dialogue Graph Modeling for Conversational Machine Reading","venue":"Findings","year":2020,"referenceCount":44,"citationCount":24,"influentialCitationCount":4,"publicationDate":"30/12/2020","authors":"Siru Ouyang,Zhuosheng Zhang,Hai Zhao","id":"114be5db62209a0d0682279f5a054a316f56697e","summary":"This work proposes a dialogue graph modeling framework by incorporating two complementary graph models, i.e., explicit discourse graph and implicit discourse graph, which respectively capture explicit and implicit interactions hidden in the rule documents.","score":1},{"url":"https://www.semanticscholar.org/paper/639eb8f1c663e18930ae2a55abe7bfd1d836ec16","title":"Multi-Turn Dialogue Reading Comprehension With Pivot Turns and Knowledge","venue":"IEEE/ACM Transactions on Audio Speech and Language Processing","year":2021,"referenceCount":61,"citationCount":8,"influentialCitationCount":2,"publicationDate":"10/02/2021","authors":"Zhuosheng Zhang,Junlong Li,Hai Zhao","id":"639eb8f1c663e18930ae2a55abe7bfd1d836ec16","summary":"This work proposes a pivot-oriented deep selection model (PoDS) on top of the Transformer-based language models for dialogue comprehension by extracting substantially important turns as pivot utterances and utilizing external knowledge to enhance the representation of context.","score":1},{"url":"https://www.semanticscholar.org/paper/281b4a7e7fb057d8266ec0610888905c46fd715d","title":"Advances in Multi-turn Dialogue Comprehension: A Survey","venue":"ArXiv","year":2021,"referenceCount":106,"citationCount":10,"influentialCitationCount":2,"publicationDate":"04/03/2021","authors":"Zhuosheng Zhang,Hai Zhao","id":"281b4a7e7fb057d8266ec0610888905c46fd715d","summary":"The characteristics and challenges of dialogue comprehension in contrast to plaintext reading comprehension are summarized and three typical patterns of dialogue modeling that are widely-used in dialogue comprehension tasks such as response selection and conversation questionanswering are discussed.","score":1},{"url":"https://www.semanticscholar.org/paper/dffedd7dcacb2fab0af708b9a6a6de8424fe2fc2","title":"Probing Commonsense Explanation in Dialogue Response Generation","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":45,"citationCount":8,"influentialCitationCount":1,"publicationDate":"19/04/2021","authors":"Pei Zhou,Pegah Jandaghi,Bill Yuchen Lin,Justin Cho,J. Pujara,Xiang Ren","id":"dffedd7dcacb2fab0af708b9a6a6de8424fe2fc2","summary":"This study formalizes the problem by framing commonsense as a latent variable in the RG task and using explanations for responses as textual form of commonsense, and collecting 6k annotated explanations justifying responses from four dialogue datasets and asking humans to verify them.","score":1},{"url":"https://www.semanticscholar.org/paper/a210df43018c682f6f57120cdb66b93a42c26699","title":"Probing Causal Common Sense in Dialogue Response Generation","venue":"ArXiv","year":2021,"referenceCount":46,"citationCount":2,"influentialCitationCount":0,"publicationDate":2021,"authors":"Pei Zhou,Pegah Jandaghi,Bill Yuchen Lin,Hyundong Justin Cho,J. Pujara,Xiang Ren","id":"a210df43018c682f6f57120cdb66b93a42c26699","summary":"The study if response generation models can emulate human reasoning process and use common sense to help produce better-quality responses finds that RG models have a hard time determining the logical validity of explanations but can identify grammatical naturalness of the explanation easily.","score":1},{"url":"https://www.semanticscholar.org/paper/5d6800ae2f543e2e7135071bdc24a3266c70570d","title":"Problems and Countermeasures in Natural Language Processing Evaluation","venue":"ArXiv","year":2021,"referenceCount":65,"citationCount":1,"influentialCitationCount":0,"publicationDate":"20/04/2021","authors":"Qingxiu Dong,Zhifang Sui,W. Zhan,Baobao Chang","id":"5d6800ae2f543e2e7135071bdc24a3266c70570d","summary":"The human language ability evaluation standard is referred to, a series of basic principles and implementation ideas for human-like machinelanguage ability evaluation from the three aspects of reliability, difficulty and validity are proposed.","score":1},{"url":"https://www.semanticscholar.org/paper/6d3287e0de1474d3143c386c4e95cdf9437df1a3","title":"Fact-driven Logical Reasoning","venue":"ArXiv","year":2021,"referenceCount":43,"citationCount":11,"influentialCitationCount":6,"publicationDate":"21/05/2021","authors":"Siru Ouyang,Zhuosheng Zhang,Hai Zhao","id":"6d3287e0de1474d3143c386c4e95cdf9437df1a3","summary":"It is argued that the natural logic units would be the group of backbone constituents of the sentence such as the subject-verb-object formed \"facts\", covering both global and local knowledge pieces that are necessary as the basis for logical reasoning.","score":1},{"url":"https://www.semanticscholar.org/paper/a03844a1cb957feae7ded3a327cd3a445e2175ad","title":"Structural Pre-training for Dialogue Comprehension","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":63,"citationCount":14,"influentialCitationCount":0,"publicationDate":"23/05/2021","authors":"Zhuosheng Zhang,Hai Zhao","id":"a03844a1cb957feae7ded3a327cd3a445e2175ad","summary":"SPIDER, Structural PretraIned DialoguE Reader, is presented, to capture dialogue exclusive features and proposes two training objectives in addition to the original LM objectives, which regularizes the model to improve the factual correctness of summarized subject-verb-object triplets.","score":1},{"url":"https://www.semanticscholar.org/paper/70a3b5c4d46490a35ee544306de06cf8c7c0e787","title":"A taxonomy, data set, and benchmark for detecting and classifying malevolent dialogue responses","venue":"J. Assoc. Inf. Sci. Technol.","year":2021,"referenceCount":108,"citationCount":4,"influentialCitationCount":1,"publicationDate":"26/05/2021","authors":"Yangjun Zhang,Pengjie Ren,M. de Rijke","id":"70a3b5c4d46490a35ee544306de06cf8c7c0e787","summary":"This work defines the task and presents a hierarchical malevolent dialogue taxonomy and applies state‐of‐the‐art text classification methods to the MDRDC task, and reports on experiments aimed at assessing the performance of these approaches.","score":1},{"url":"https://www.semanticscholar.org/paper/a8debd8f58ee690005d996d223c37239e25273ec","title":"CIDER: Commonsense Inference for Dialogue Explanation and Reasoning","venue":"SIGDIAL Conferences","year":2021,"referenceCount":31,"citationCount":6,"influentialCitationCount":1,"publicationDate":"01/06/2021","authors":"Deepanway Ghosal,Pengfei Hong,Siqi Shen,Navonil Majumder,Rada Mihalcea,Soujanya Poria","id":"a8debd8f58ee690005d996d223c37239e25273ec","summary":"This work introduces CIDER – a manually curated dataset that contains dyadic dialogue explanations in the form of implicit and explicit knowledge triplets inferred using contextual commonsense inference that can be conducive to improving several downstream applications.","score":1},{"url":"https://www.semanticscholar.org/paper/4ca39cf99747b8962fe37e7e025e284872df3425","title":"Comparing Test Sets with Item Response Theory","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":66,"citationCount":9,"influentialCitationCount":0,"publicationDate":"01/06/2021","authors":"Clara Vania,Phu Mon Htut,William Huang,Dhara Mungra,Richard Yuanzhe Pang,Jason Phang,Haokun Liu,Kyunghyun Cho,Sam Bowman","id":"4ca39cf99747b8962fe37e7e025e284872df3425","summary":"Quoref, HellaSwag, and MC-TACO are best suited for distinguishing among state-of-the-art models, while SNLI, MNLI, and CommitmentBank seem to be saturated for current strong models.","score":1},{"url":"https://www.semanticscholar.org/paper/976a5d4acfd761e236fe54a92b269239084ec5f4","title":"Multi -Turn Response Selection with Temporal Gated Graph Convolutional Networks","venue":"IEEE International Joint Conference on Neural Network","year":2021,"referenceCount":28,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/07/2021","authors":"Siyu Tao,Qiang Zhao,Linlin Wang,Liang He","id":"976a5d4acfd761e236fe54a92b269239084ec5f4","summary":"A novel graph-based retrieval model that first construct a temporal graph based on both dialogue contexts and utterance relations, and then leverage the gated graph convolutional networks to aggregate significant information from all neighboring utterances to perform accurate reasoning over multi-turn dialogues.","score":1},{"url":"https://www.semanticscholar.org/paper/614cfe0d9edb0ad9caa5a446fbbde95af5afe9ac","title":"A Survey on Response Selection for Retrieval-based Dialogues","venue":"International Joint Conference on Artificial Intelligence","year":2021,"referenceCount":68,"citationCount":11,"influentialCitationCount":0,"publicationDate":"01/08/2021","authors":"Chongyang Tao,Jiazhan Feng,Rui Yan,Wei Wu,Daxin Jiang","id":"614cfe0d9edb0ad9caa5a446fbbde95af5afe9ac","summary":"A comprehensive survey of recent advances in response selection for retrieval-based dialogues and summarizes some recent advances on the research of response selection, including incorporation with extra knowledge and exploration on more effective model learning.","score":1},{"url":"https://www.semanticscholar.org/paper/6d62cd2611b972bc2a9b3c7d4a133fbad0984afe","title":"Validation on machine reading comprehension software without annotated labels: a property-based method","venue":"ESEC/SIGSOFT FSE","year":2021,"referenceCount":70,"citationCount":8,"influentialCitationCount":1,"publicationDate":"18/08/2021","authors":"Songqiang Chen,Shuo Jin,Xiaoyuan Xie","id":"6d62cd2611b972bc2a9b3c7d4a133fbad0984afe","summary":"A property- based validation method for MRC software with Metamorphic Testing to supplement the reference-based validation that does not refer to the labels and hence can make much data available for testing and reveal problems that have been concealed by the traditional validation.","score":1},{"url":"https://www.semanticscholar.org/paper/52c5c9575ebd990ed34867708dd42aa8ba9d561f","title":"Smoothing Dialogue States for Open Conversational Machine Reading","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":45,"citationCount":5,"influentialCitationCount":2,"publicationDate":"28/08/2021","authors":"Zhuosheng Zhang,Siru Ouyang,Hai Zhao,M. Utiyama,E. Sumita","id":"52c5c9575ebd990ed34867708dd42aa8ba9d561f","summary":"This work proposes an effective gating strategy by smoothing the two dialogue states in only one decoder and bridge decision making and question generation to provide a richer dialogue state reference and achieves new state-of-the-art results.","score":1},{"url":"https://www.semanticscholar.org/paper/53baebc368d2b7943c7c4fd56a3716ccfc472d51","title":"Self- and Pseudo-self-supervised Prediction of Speaker and Key-utterance for Multi-party Dialogue Reading Comprehension","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":39,"citationCount":6,"influentialCitationCount":1,"publicationDate":"08/09/2021","authors":"Yiyang Li,Hai Zhao","id":"53baebc368d2b7943c7c4fd56a3716ccfc472d51","summary":"Two labour-free self- and pseudo-self-supervised prediction tasks on speaker and key-utterance to implicitly model the speaker information, and capture salient clues in a long dialogue are designed.","score":1},{"url":"https://www.semanticscholar.org/paper/81b58944372ea10436ff7252b115e21e893d11c6","title":"Enhanced Speaker-aware Multi-party Multi-turn Dialogue Comprehension","venue":"ArXiv","year":2021,"referenceCount":49,"citationCount":5,"influentialCitationCount":0,"publicationDate":"09/09/2021","authors":"Xinbei Ma,Zhuosheng Zhang,Hai Zhao","id":"81b58944372ea10436ff7252b115e21e893d11c6","summary":"An enhanced speaker-aware model with masking attention and heterogeneous graph networks to comprehensively capture discourse clues from both sides of speaker property and speaker- Aware relationships is proposed.","score":1},{"url":"https://www.semanticscholar.org/paper/0a729d180bdf126bf0f350d7ac7ec2b2157eefa6","title":"An Evaluation Dataset and Strategy for Building Robust Multi-turn Response Selection Model","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":26,"citationCount":3,"influentialCitationCount":1,"publicationDate":"10/09/2021","authors":"Kijong Han,Seojin Lee,Wooin Lee,Joosung Lee,Donghun Lee","id":"0a729d180bdf126bf0f350d7ac7ec2b2157eefa6","summary":"The weaknesses of the open-domain Korean Multi-turn response selection models are analyzed and an adversarial dataset is published to evaluate these weaknesses and a strategy to build a robust model in this adversarial environment is suggested.","score":1},{"url":"https://www.semanticscholar.org/paper/2820c2f6147ca8dbc19181fa712b2662dd0c3ae0","title":"Guiding Topic Flows in the Generative Chatbot by Enhancing the ConceptNet with the Conversation Corpora","venue":"ArXiv","year":2021,"referenceCount":40,"citationCount":0,"influentialCitationCount":0,"publicationDate":"12/09/2021","authors":"Pengda Si,Yao Qiu,Jinchao Zhang,Yujiu Yang","id":"2820c2f6147ca8dbc19181fa712b2662dd0c3ae0","summary":"This work proposes the method to supply more 017 concept relations extracted from the conversa- 018 tional corpora and build an enhanced concept 019 graph for the chatbot construction, which significantly outperforms strong baseline systems and achieves new SOTA results.","score":1},{"url":"https://www.semanticscholar.org/paper/2a3dd5cf961747adcb05f4f2834ff7a22261e861","title":"Commonsense-Focused Dialogues for Response Generation: An Empirical Study","venue":"SIGDIAL Conferences","year":2021,"referenceCount":45,"citationCount":18,"influentialCitationCount":2,"publicationDate":"14/09/2021","authors":"Pei Zhou,Karthik Gopalakrishnan,Behnam Hedayatnia,Seokhwan Kim,J. Pujara,Xiang Ren,Yang Liu,Dilek Z. Hakkani-Tür","id":"2a3dd5cf961747adcb05f4f2834ff7a22261e861","summary":"This paper auto-extract commonsensical dialogues from existing dialogue datasets by leveraging ConceptNet, a commonsense knowledge graph, and proposes an approach for automatic evaluation of commonsense that relies on features derived from ConceptNet and pre-trained language and dialog models, and shows reasonable correlation with human evaluation of responses’ commonsense quality.","score":1},{"url":"https://www.semanticscholar.org/paper/cef565dfb89aaa30191ec359c5cf7ca2cbc129fd","title":"FCM: A Fine-grained Comparison Model for Multi-turn Dialogue Reasoning","venue":"Conference on Empirical Methods in Natural Language Processing","year":2021,"referenceCount":32,"citationCount":4,"influentialCitationCount":0,"publicationDate":"22/09/2021","authors":"Xu Wang,Hainan Zhang,Shuai Zhao,Yanyan Zou,Hongshen Chen,Zhuoye Ding,Bo Cheng,Yanyan Lan","id":"cef565dfb89aaa30191ec359c5cf7ca2cbc129fd","summary":"Inspired by human’s behavior in reading comprehension, a comparison mechanism is proposed to focus on the fine-grained differences in the representation of each response candidate, to tackle the problem of logical consistency in multi-turn dialogue reasoning.","score":1},{"url":"https://www.semanticscholar.org/paper/ce74df5126faad7d74f578f1e1953278611e235d","title":"Think Before You Speak: Using Self-talk to Generate Implicit Commonsense Knowledge for Response Generation","venue":"ArXiv","year":2021,"referenceCount":60,"citationCount":6,"influentialCitationCount":0,"publicationDate":2021,"authors":"Pei Zhou,Karthik Gopalakrishnan,Behnam Hedayatnia,Seokhwan Kim,J. Pujara,Xiang Ren,Yang Liu,Dilek Z. Hakkani-Tür","id":"ce74df5126faad7d74f578f1e1953278611e235d","summary":"This paper presents a self-talk approach that first generates the implicit commonsense knowledge and then generates response by referencing the externalized knowledge, all using one generative model.","score":1},{"url":"https://www.semanticscholar.org/paper/b2785368eea52d1cdfc225fc6268f84a1831c350","title":"Modeling Hierarchical Reasoning Chains by Linking Discourse Units and Key Phrases for Reading Comprehension","venue":"International Conference on Computational Linguistics","year":2022,"referenceCount":39,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Jialin Chen,Zhuosheng Zhang,Hai Zhao","id":"b2785368eea52d1cdfc225fc6268f84a1831c350","summary":"A holistic graph network (HGN) that deals with context at both discourse-level and word-level as the basis for logical reasoning to provide a more fine-grained relation extraction and is modeled by a hierarchical interaction mechanism to improve the interpretation of MRC systems.","score":1},{"url":"https://www.semanticscholar.org/paper/28144bdbeeff5f86c93a2b22b30c2ad210305cca","title":"RobustLR: Evaluating Robustness to Logical Perturbation in Deductive Reasoning","venue":"ArXiv","year":2022,"referenceCount":27,"citationCount":3,"influentialCitationCount":0,"publicationDate":"25/05/2022","authors":"Soumya Sanyal,Zeyi Liao,Xiang Ren","id":"28144bdbeeff5f86c93a2b22b30c2ad210305cca","summary":"R OBUST LR is proposed, a suite of evaluation datasets that evaluate the robustness of Transformers models to minimal logical edits in rulebases and some standard logical equivalence conditions, and demonstrates some short-comings of the deductive reasoning-based language models.","score":1},{"url":"https://www.semanticscholar.org/paper/c8559021289f08eaf8cf2294e406bc1c6b506d19","title":"Recent Advances in Deep Learning Based Dialogue Systems: A Systematic Survey","venue":"Artificial Intelligence Review","year":2021,"referenceCount":480,"citationCount":60,"influentialCitationCount":3,"publicationDate":"10/05/2021","authors":"Jinjie Ni,Tom Young,Vlad Pandelea,Fuzhao Xue,V. Adiga,E. Cambria","id":"c8559021289f08eaf8cf2294e406bc1c6b506d19","summary":"This survey is the most comprehensive and up-to-date one at present in the area of dialogue systems and dialogue-related tasks, extensively covering the popular frameworks, topics, and datasets.","score":1},{"url":"https://www.semanticscholar.org/paper/6f1c10534f6407ef3b090032b4dc2f9073569526","title":"Think Before You Speak: Explicitly Generating Implicit Commonsense Knowledge for Response Generation","venue":"Annual Meeting of the Association for Computational Linguistics","year":2021,"referenceCount":69,"citationCount":6,"influentialCitationCount":0,"publicationDate":"16/10/2021","authors":"Pei Zhou,Karthik Gopalakrishnan,Behnam Hedayatnia,Seokhwan Kim,J. Pujara,Xiang Ren,Yang Liu,Dilek Z. Hakkani-Tür","id":"6f1c10534f6407ef3b090032b4dc2f9073569526","summary":"Think-Before-Speaking is presented, a generative approach to first externalize implicit commonsense knowledge (think) and use this knowledge to generate responses (speak), arguing that externalizing implicit knowledge allows more efficient learning, produces more informative responses, and enables more explainable models.","score":1},{"url":"https://www.semanticscholar.org/paper/2ae757afd718d5219cdee3a6c4cee0d226378efd","title":"Representation Learning for Conversational Data using Discourse Mutual Information Maximization","venue":"North American Chapter of the Association for Computational Linguistics","year":2021,"referenceCount":50,"citationCount":1,"influentialCitationCount":0,"publicationDate":"04/12/2021","authors":"Bishal Santra,Sumegh Roychowdhury,Aishik Mandal,Vasu Gurram,Atharva Naik,Manish Gupta,Pawan Goyal","id":"2ae757afd718d5219cdee3a6c4cee0d226378efd","summary":"This work proposes a structure-aware Mutual Information based loss-function DMI (Discourse Mutual Information) for training dialog-representation models, that additionally captures the inherent uncertainty in response prediction.","score":1},{"url":"https://www.semanticscholar.org/paper/3c9ba25baca64151af4e9d50c7947de28eb2a599","title":"Survey of Hallucination in Natural Language Generation","venue":"ACM Computing Surveys","year":2022,"referenceCount":250,"citationCount":50,"influentialCitationCount":4,"publicationDate":"08/02/2022","authors":"Ziwei Ji,Nayeon Lee,Rita Frieske,Tiezheng Yu,D. Su,Yan Xu,Etsuko Ishii,Yejin Bang,Wenliang Dai,Andrea Madotto,Pascale Fung","id":"3c9ba25baca64151af4e9d50c7947de28eb2a599","summary":"A broad overview of the research progress and challenges in the hallucination problem in NLG is provided, including task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, and machine translation.","score":1},{"url":"https://www.semanticscholar.org/paper/242eaa55cce5daad200850dd10a788a0f960cdd8","title":"Logical Reasoning for Task Oriented Dialogue Systems","venue":"ECNLP","year":2022,"referenceCount":32,"citationCount":1,"influentialCitationCount":0,"publicationDate":"08/02/2022","authors":"Sajjad Beygi,M. Fazel-Zarandi,Alessandra Cervone,Prakash Krishnan,Siddhartha R. Jonnalagadda","id":"242eaa55cce5daad200850dd10a788a0f960cdd8","summary":"This work proposes a novel method to fine-tune pretrained transformer models such as Roberta and T5, to reason over a set of facts in a given dialogue context, and shows that the transformer based model can perform logical reasoning to answer questions when the dialogue context contains all the required information.","score":1},{"url":"https://www.semanticscholar.org/paper/d179082956ab75d08311ddc1bbb20783031d15b1","title":"Leveraging speaker-aware structure and factual knowledge for faithful dialogue summarization","venue":"Knowledge-Based Systems","year":2022,"referenceCount":75,"citationCount":2,"influentialCitationCount":0,"publicationDate":"01/03/2022","authors":"Lulu Zhao,Weiran Xu,Chunyun Zhang,Jun Guo","id":"d179082956ab75d08311ddc1bbb20783031d15b1","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/3c05f71157c713fe45704bdd130f01620b7ab771","title":"Towards Robust Online Dialogue Response Generation","venue":"ArXiv","year":2022,"referenceCount":53,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/03/2022","authors":"Leyang Cui,Fandong Meng,Yanjun Liu,Jie Zhou,Yue Zhang","id":"3c05f71157c713fe45704bdd130f01620b7ab771","summary":"A hierarchical sampling-based method consist- of both utterance-level sampling and semi- 017 utterance -level sampling, to alleviate the dis- 018 crepancy, which implicitly increases the dia- 019 logue coherence.","score":1},{"url":"https://www.semanticscholar.org/paper/596b1f054db22bdd148676cdfcce26d22c2c14cb","title":"Learning-by-Narrating: Narrative Pre-Training for Zero-Shot Dialogue Comprehension","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":29,"citationCount":1,"influentialCitationCount":0,"publicationDate":"19/03/2022","authors":"Chao Zhao,Wenlin Yao,Dian Yu,Kaiqiang Song,Dong Yu,Jianshu Chen","id":"596b1f054db22bdd148676cdfcce26d22c2c14cb","summary":"A novel narrative-guided pre-training strategy that learns by narrating the key information from a dialogue input by automatically aligning movie subtitles and their synopses is developed and Experimental results show that the model achieves superior zero-shot performance but also exhibits stronger fine-grained dialogue comprehension capabilities.","score":1},{"url":"https://www.semanticscholar.org/paper/0f17d7619e5de7bf41079d65783d4fb135825377","title":"CICERO: A Dataset for Contextualized Commonsense Inference in Dialogues","venue":"Annual Meeting of the Association for Computational Linguistics","year":2022,"referenceCount":34,"citationCount":6,"influentialCitationCount":1,"publicationDate":"25/03/2022","authors":"Deepanway Ghosal,Siqi Shen,Navonil Majumder,Rada Mihalcea,Soujanya Poria","id":"0f17d7619e5de7bf41079d65783d4fb135825377","summary":"This paper curates CICERO, a dataset of dyadic conversations with five types of utterance-level reasoning-based inferences, and uses it to solve relevant generative and discriminative tasks: generation of cause and subsequent event; generation of prerequisite, motivation, and listener’s emotional reaction; and selection of plausible alternatives.","score":1},{"url":"https://www.semanticscholar.org/paper/706c6b3781374b0b11f98f204a4ddd05b26ed009","title":"Knowledge Infused Decoding","venue":"International Conference on Learning Representations","year":2022,"referenceCount":98,"citationCount":5,"influentialCitationCount":1,"publicationDate":"06/04/2022","authors":"Ruibo Liu,Guoqing Zheng,Shashank Gupta,Radhika Gaonkar,Chongyang Gao,Soroush Vosoughi,Milad Shokouhi,A. Awadallah","id":"706c6b3781374b0b11f98f204a4ddd05b26ed009","summary":"Knowledge Infused Decoding (KID)—a novel decoding algorithm for generative LMs, which dynamically infuses external knowledge into each step of the LM decoding, which maintains a local knowledge memory based on the current context, interacting with a dynamically created external knowledge trie, and continuously update the local memory as a knowledge-aware constraint to guide decoding via reinforcement learning.","score":1},{"url":"https://www.semanticscholar.org/paper/860552613fa7529553c4cd934b98be52c57e2528","title":"Autoencoding Language Model Based Ensemble Learning for Commonsense Validation and Explanation","venue":"ArXiv","year":2022,"referenceCount":33,"citationCount":0,"influentialCitationCount":0,"publicationDate":"07/04/2022","authors":"Ngo Quang Huy,Tu Minh Phuong,Ngo Xuan Bach","id":"860552613fa7529553c4cd934b98be52c57e2528","summary":"AlMEn, an advanced pre-trained language models based method for commonsense validation and explanation based on Siamese neural net-works, can distinguish natural language statements that are against commonsense (validation subtask) and correctly identify the reason for making against Commonsense (explanation selection subtask).","score":1},{"url":"https://www.semanticscholar.org/paper/4cc6d310c0d5584f50836f1bd6bdbcac1c1c86a6","title":"Towards Fine-grained Causal Reasoning and QA","venue":"ArXiv","year":2022,"referenceCount":61,"citationCount":2,"influentialCitationCount":1,"publicationDate":"15/04/2022","authors":"Linyi Yang,Zhen Wang,Yu-Xin Wu,Jie Yang,Yue Zhang","id":"4cc6d310c0d5584f50836f1bd6bdbcac1c1c86a6","summary":"A novelne-grained causal reasoning dataset is introduced and a series of novel predictive tasks in NLP, such as causality detection, event causality extraction, and Causal QA are presented, to highlight potential research opportunities.","score":1},{"url":"https://www.semanticscholar.org/paper/7ba28d214d98f2a9c2e37e6cdf294d0d4e2a1e50","title":"Back to the Future: Bidirectional Information Decoupling Network for Multi-turn Dialogue Modeling","venue":"ArXiv","year":2022,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/04/2022","authors":"Yiyang Li,Hai Zhao,Zhuosheng Zhang","id":"7ba28d214d98f2a9c2e37e6cdf294d0d4e2a1e50","summary":"This work proposes Bidirectional Information Decoupling Network (BiDeN) as a universal dialogue encoder, which explicitly incorporates both the past and future contexts and can be generalized to a wide range of dialogue-related tasks.","score":1},{"url":"https://www.semanticscholar.org/paper/8b78827faf49277b8f9f4510a766cba30e5fbe20","title":"LogiGAN: Learning Logical Reasoning via Adversarial Pre-training","venue":"ArXiv","year":2022,"referenceCount":82,"citationCount":2,"influentialCitationCount":0,"publicationDate":"18/05/2022","authors":"Xinyu Pi,Wanjun Zhong,Yan Gao,Nan Duan,Jian-Guang Lou","id":"8b78827faf49277b8f9f4510a766cba30e5fbe20","summary":"LogiGAN is presented, an unsupervised adversarial pre-training framework for improving logical reasoning abilities of language models and ablation studies on LogiGAN components reveal the relative orthogonality between linguistic and logic abilities and suggest that reﬂective thinking’s facilitation effect might also generalize to machine learning.","score":1},{"url":"https://www.semanticscholar.org/paper/a6880a4c3f4b2f0a1d492d689569683ffbc03076","title":"DFM: Dialogue Foundation Model for Universal Large-Scale Dialogue-Oriented Task Learning","venue":"","year":2022,"referenceCount":92,"citationCount":1,"influentialCitationCount":0,"publicationDate":"25/05/2022","authors":"Zhi Chen,Jijia Bao,Lu Chen,Yuncong Liu,Da Ma,B. Chen,Mengyue Wu,Su Zhu,Xinhsuai Dong,Fujiang Ge,Qingliang Miao,Jian-Guang Lou,Kai Yu","id":"a6880a4c3f4b2f0a1d492d689569683ffbc03076","summary":"Experiments show that, compared with models of the same size, DFM can achieve state-of-the-art or competitive performance on very rich cross-domain downstream dialogue tasks, and demonstrates that DFM largely ex-tends the ability of uniﬁed dialogue pre-trained model.","score":1},{"url":"https://www.semanticscholar.org/paper/8f926c0c3f1557a9241b7e75609082a1f207a75e","title":"Improving Zero and Few-shot Generalization in Dialogue through Instruction Tuning","venue":"ArXiv","year":2022,"referenceCount":121,"citationCount":10,"influentialCitationCount":0,"publicationDate":"25/05/2022","authors":"Prakhar Gupta,Cathy Jiao,Yi-Ting Yeh,Shikib Mehri,M. Eskénazi,Jeffrey P. Bigham","id":"8f926c0c3f1557a9241b7e75609082a1f207a75e","summary":"This work introduces InstructDial, an instruction tuning framework for dialogue, which consists of a repository of 48 diverse dialogue tasks in a unified text-to-text format created from 59 openly available dialogue datasets, and introduces novel meta-tasks to ensure that models adhere to instructions.","score":1},{"url":"https://www.semanticscholar.org/paper/7e582b03b597d8865f6641c511c1a63b6255b821","title":"DialogZoo: Large-Scale Dialog-Oriented Task Learning","venue":"ArXiv","year":2022,"referenceCount":86,"citationCount":3,"influentialCitationCount":0,"publicationDate":2022,"authors":"Zhi Chen,Jijia Bao,Lu Chen,Yuncong Liu,Da Ma,B. Chen,Mengyue Wu,Su Zhu,Jian-Guang Lou,Kai Yu","id":"7e582b03b597d8865f6641c511c1a63b6255b821","summary":"The experimental results show that the building a uniﬁed foundation model which can solve massive diverse dialogue tasks and improves the ability of dialogue generation and knowledge distillation, but also the representation ability of models.","score":1},{"url":"https://www.semanticscholar.org/paper/663de4d82f1f46770aea50f5c51b424b1f4761df","title":"Discourse-Aware Graph Networks for Textual Logical Reasoning","venue":"ArXiv","year":2022,"referenceCount":117,"citationCount":0,"influentialCitationCount":0,"publicationDate":"04/07/2022","authors":"Yinya Huang,Lemao Liu,Kun Xu,Meng Fang,Liang Lin,Xi Liang","id":"663de4d82f1f46770aea50f5c51b424b1f4761df","summary":"This work proposes logic structural-constraint modeling to solve the logical reasoning QA and introduces discourse-aware graph networks (DAGNs), which demonstrate the reasonability of the logical structures built in DAGNs and the effectiveness of the learned logic features.","score":1},{"url":"https://www.semanticscholar.org/paper/7d3895652d37242f0f2214b6963b09c04273b923","title":"Semantic-based Pre-training for Dialogue Understanding","venue":"International Conference on Computational Linguistics","year":2022,"referenceCount":72,"citationCount":2,"influentialCitationCount":0,"publicationDate":"19/09/2022","authors":"Xuefeng Bai,Linfeng Song,Yue Zhang","id":"7d3895652d37242f0f2214b6963b09c04273b923","summary":"A semantic-based pre-training framework that extends the standard pre- training framework by three tasks for learning 1) core semantic units, 2) semantic relations and 3) the overall semantic representation according to AMR graphs is proposed.","score":1},{"url":"https://www.semanticscholar.org/paper/4d991660c5e34dec1a3f26ffcc28d4a3f3d5263e","title":"ET5: A Novel End-to-end Framework for Conversational Machine Reading Comprehension","venue":"International Conference on Computational Linguistics","year":2022,"referenceCount":24,"citationCount":1,"influentialCitationCount":0,"publicationDate":"23/09/2022","authors":"Xiao Zhang,Heyan Huang,Zewen Chi,Xian-Ling Mao","id":"4d991660c5e34dec1a3f26ffcc28d4a3f3d5263e","summary":"A novel end-to-end framework for conversational machine reading comprehension based on shared parameter mechanism, called entailment reasoning T5 (ET5), is proposed, which achieves new state-of-the-art results on the ShARC leaderboard with the BLEU-4 score of 55.2.","score":1},{"url":"https://www.semanticscholar.org/paper/d0ba95d3c7766038ea47fda8a13377cf3ee1c8e3","title":"Multiview Contextual Commonsense Inference: A New Dataset and Task","venue":"ArXiv","year":2022,"referenceCount":25,"citationCount":1,"influentialCitationCount":0,"publicationDate":"06/10/2022","authors":"Siqi Shen,Deepanway Ghosal,Navonil Majumder,Henry Lim,Rada Mihalcea,Soujanya Poria","id":"d0ba95d3c7766038ea47fda8a13377cf3ee1c8e3","summary":"This work creates CICEROv2, a dataset consisting of 8,351 instances from 2,379 dialogues, containing multiple human-written answers for each contextual commonsense inference question, representing a type of explanation on cause, subsequent event, motivation, and emotional reaction.","score":1},{"url":"https://www.semanticscholar.org/paper/f74a7c4a2d0a0fe41a77357ce11a519f5b059dee","title":"Towards End-to-End Open Conversational Machine Reading","venue":"ArXiv","year":2022,"referenceCount":28,"citationCount":0,"influentialCitationCount":0,"publicationDate":"13/10/2022","authors":"Sizhe Zhou,Siru Ouyang,Zhuosheng Zhang,Hai Zhao Department of Computer Science,Engineering,S. University,Key Laboratory of Shanghai Education Commission for In Interaction,Cognitive Engineering,Moe Intelligence,AI Institute","id":"f74a7c4a2d0a0fe41a77357ce11a519f5b059dee","summary":"This work model OR-CMR as a unified text-to-text task in a fully end-toend style and shows the effectiveness of the proposed end- to-end framework on both sub-tasks by a large margin, achieving new state-of-theart results.","score":1},{"url":"https://www.semanticscholar.org/paper/fa71d25c07d6d3c890ef4b7547d5a4d117d0b96d","title":"Taxonomy of Abstractive Dialogue Summarization: Scenarios, Approaches and Future Directions","venue":"ArXiv","year":2022,"referenceCount":159,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/10/2022","authors":"Qi Jia,Siyu Ren,Yizhu Liu,Kenny Q. Zhu","id":"fa71d25c07d6d3c890ef4b7547d5a4d117d0b96d","summary":"This survey provides a comprehensive investigation on existing work for abstractive dialogue summarization from scenarios, approaches to evaluations and presents a taxonomy of existing techniques in three directions, namely, injecting dialogue features, designing auxiliary training tasks and using additional data.","score":1},{"url":"https://www.semanticscholar.org/paper/70f1419c0778350abb75ff884691b9933e408888","title":"Disentangling Reasoning Capabilities from Language Models with Compositional Reasoning Transformers","venue":"ArXiv","year":2022,"referenceCount":42,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/10/2022","authors":"Wanjun Zhong,Tingting Ma,Jia-hai Wang,Jian Yin,T. Zhao,Chin-Yew Lin,Nan Duan","id":"70f1419c0778350abb75ff884691b9933e408888","summary":"ReasonFormer, a uniﬁed reasoning framework for mirroring the modular and compositional reasoning process of humans in complex decision-making, demonstrates substantial performance boosts and shows the modularity of reasoning modules as different tasks activate distinct reasoning skills at different reasoning depths.","score":1},{"url":"https://www.semanticscholar.org/paper/2b22a3acb3ba1581d320b70b02343d4a0f356e3e","title":"MetaLogic: Logical Reasoning Explanations with Fine-Grained Structure","venue":"ArXiv","year":2022,"referenceCount":25,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/10/2022","authors":"Yinya Huang,Hongming Zhang,Ruixin Hong,Xiaodan Liang,Changshui Zhang,Dong Yu","id":"2b22a3acb3ba1581d320b70b02343d4a0f356e3e","summary":"The experimental results show that generating reasoning graphs remains a challenging task for current models, even with the help of giant pre-trained language models on the proposed comprehensive logical reasoning explanation form.","score":1},{"url":"https://www.semanticscholar.org/paper/d1feb79f63ea52839f4a784fbd7d60bb73dd98dd","title":"ComFact: A Benchmark for Linking Contextual Commonsense Knowledge","venue":"ArXiv","year":2022,"referenceCount":67,"citationCount":2,"influentialCitationCount":0,"publicationDate":"23/10/2022","authors":"Silin Gao,Jena D. Hwang,Saya Kanno,Hiromi Wakaki,Yuki Mitsufuji,Antoine Bosselut","id":"d1feb79f63ea52839f4a784fbd7d60bb73dd98dd","summary":"This work proposes the new task of commonsense fact linking, where models are given contexts and trained to identify situationally-relevant commonsense knowledge from KGs, and demonstrates across-the-board performance improvements over simple heuristics.","score":1},{"url":"https://www.semanticscholar.org/paper/36b1bb9d1149586ae4e9a1867a179760051028e9","title":"DialogConv: A Lightweight Fully Convolutional Network for Multi-view Response Selection","venue":"ArXiv","year":2022,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"publicationDate":"25/10/2022","authors":"Yongkang Liu,Shi Feng,W. Gao,Daling Wang,Yifei Zhang","id":"36b1bb9d1149586ae4e9a1867a179760051028e9","summary":"This paper proposes a novel lightweight fully convolutional architecture, called DialogConv, for response selection, exclusively built on top of convolution to extract matching features of context and response.","score":1},{"url":"https://www.semanticscholar.org/paper/f7b2ff0dc7022d67a15ff5df594058587091fc6f","title":"Improving conversational search with query reformulation using selective contextual history","venue":"Data and Information Management","year":2022,"referenceCount":30,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/11/2022","authors":"Haya Al-Thani,T. Elsayed,B. Jansen","id":"f7b2ff0dc7022d67a15ff5df594058587091fc6f","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/54a3f23363bf28c11f98a135ace5167dd5f0d51e","title":"Channel-aware Decoupling Network for Multi-turn Dialogue Comprehension","venue":"IEEE Transactions on Neural Networks and Learning Systems","year":2022,"referenceCount":78,"citationCount":0,"influentialCitationCount":0,"publicationDate":"14/11/2022","authors":"Zhuosheng Zhang,Hai Zhao,Longxiang Liu","id":"54a3f23363bf28c11f98a135ace5167dd5f0d51e","summary":"This work proposes compositional learning for holistic interaction across the utterances beyond the sequential contextualization from PrLMs, in order to capture the utterance-aware and speaker-aware representations entailed in a dialog history.","score":1},{"url":"https://www.semanticscholar.org/paper/ddbbf78334ef65d412520236e5d9067c82ef8984","title":"Reflect, Not Reflex: Inference-Based Common Ground Improves Dialogue Response Quality","venue":"ArXiv","year":2022,"referenceCount":37,"citationCount":1,"influentialCitationCount":0,"publicationDate":"16/11/2022","authors":"Pei Zhou,Hyundong Justin Cho,Pegah Jandaghi,Dong-Ho Lee,Bill Yuchen Lin,J. Pujara,Xiang Ren","id":"ddbbf78334ef65d412520236e5d9067c82ef8984","summary":"Surprisingly, it is found that simply prompting GPT3 to “think” about CG generates 30% more quality responses, showing promising benefits to integrating CG into the RG process.","score":1},{"url":"https://www.semanticscholar.org/paper/ed4e9a69594aee3c39a02b2f4fe1a614bc156da5","title":"IRRGN: An Implicit Relational Reasoning Graph Network for Multi-turn Response Selection","venue":"ArXiv","year":2022,"referenceCount":45,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/12/2022","authors":"Jing-Hui Deng,Hengwei Dai,Xuewei Guo,Yuanchen Ju,Wei Peng","id":"ed4e9a69594aee3c39a02b2f4fe1a614bc156da5","summary":"The Utterance Relational Reasoner (URR) and the Option Dual Comparator (ODC) are proposed, which aims to implicitly extract dependencies between utterances, as well as utterances and options, and make reasoning with relational graph convolutional networks.","score":1},{"url":"https://www.semanticscholar.org/paper/52fe39b8d4d59f42ef22bc375d169c891748f819","title":"HiBERT: Detecting the illogical patterns with hierarchical BERT for multi-turn dialogue reasoning","venue":"Neurocomputing","year":2022,"referenceCount":26,"citationCount":0,"influentialCitationCount":0,"publicationDate":"01/12/2022","authors":"Xu Wang,Hainan Zhang,Shuai Zhao,Hongshen Chen,Bo Cheng,Zhuoye Ding,Sulong Xu,Weipeng P. Yan,Yanyan Lan","id":"52fe39b8d4d59f42ef22bc375d169c891748f819","summary":"","score":1},{"url":"https://www.semanticscholar.org/paper/493a6e7aef4ead8fafa8913ce404a870d862c08b","title":"Mind the Knowledge Gap: A Survey of Knowledge-enhanced Dialogue Systems","venue":"ArXiv","year":2022,"referenceCount":159,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Sagi Shaier,L. Hunter,Katharina Kann","id":"493a6e7aef4ead8fafa8913ce404a870d862c08b","summary":"This work surveys the motivation for enhancing DSs with knowledge, used datasets, and methods for knowledge search, knowledge encoding, and knowledge incorporation, and proposes how to improve existing systems based on theories from linguistics and cognitive science.","score":1},{"url":"https://www.semanticscholar.org/paper/105cbcf6806a868d1d933305e210715e807eac30","title":"Bridging The Gap: Entailment Fused-T5 for Open-retrieval Conversational Machine Reading Comprehension","venue":"ArXiv","year":2022,"referenceCount":19,"citationCount":0,"influentialCitationCount":0,"publicationDate":"19/12/2022","authors":"Xiao Zhang,Heyan Huang,Zewen Chi,Xian-Ling Mao","id":"105cbcf6806a868d1d933305e210715e807eac30","summary":"A novel one-stage end-to-end framework, called Entailment Fused-T5 (EFT), is proposed to bridge the information gap between decision-making and generation in a global understanding manner and achieves new state-of-the-art performance on the OR-ShARC benchmark.","score":1},{"url":"https://www.semanticscholar.org/paper/bc9d103493d93a9ad8e6b60af4d9a900e4470146","title":"CausalDialogue: Modeling Utterance-level Causality in Conversations","venue":"ArXiv","year":2022,"referenceCount":50,"citationCount":0,"influentialCitationCount":0,"publicationDate":"20/12/2022","authors":"Yi-Lin Tuan,Alon Albalak,Wenda Xu,Michael Stephen Saxon,Connor Pryor,L. Getoor,William Yang Wang","id":"bc9d103493d93a9ad8e6b60af4d9a900e4470146","summary":"This research examines user utterances as causes and generated responses as effects and proposes a causality-enhanced method called Exponential Maximum Average Treatment Effect (ExMATE) to enhance the impact of causality at the utterance level in training neural conversation models.","score":1},{"url":"https://www.semanticscholar.org/paper/8866789034c3021b80dfb4c0b2c21989aa97a8d5","title":"SAPBERT: Speaker-Aware Pretrained BERT for Emotion Recognition in Conversation","venue":"Algorithms","year":2022,"referenceCount":12,"citationCount":0,"influentialCitationCount":0,"publicationDate":"22/12/2022","authors":"Seunguook Lim,Jihie Kim","id":"8866789034c3021b80dfb4c0b2c21989aa97a8d5","summary":"A new pre-trained model is proposed, SAPBERT, that learns to identify speakers in a conversation to capture the speaker-dependent contexts and address the ERC task.","score":1},{"url":"https://www.semanticscholar.org/paper/0c003db762ad6edcdbabf178ae4e6776eb08a56e","title":"STRUDEL: Structured Dialogue Summarization for Dialogue Comprehension","venue":"ArXiv","year":2022,"referenceCount":23,"citationCount":0,"influentialCitationCount":0,"publicationDate":"24/12/2022","authors":"Borui Wang,Chengcheng Feng,Arjun Nair,Madelyn Mao,Jai Desai,Asli Celikyilmaz,Haoran Li,Yashar Mehdad,Dragomir R. Radev","id":"0c003db762ad6edcdbabf178ae4e6776eb08a56e","summary":"A novel type of dialogue summarization task - STRUctured DiaLoguE Summarization (STRUDEL) - that can help pre-trained language models to better understand dialogues and improve their performance on important dialogue comprehension tasks is proposed.","score":1},{"url":"https://www.semanticscholar.org/paper/8089bfe8aa59151147b78d9c9968026119cd5420","title":"Dialogue-adaptive language model pre-training from quality estimation☆","venue":"Neurocomputing","year":2020,"referenceCount":53,"citationCount":0,"influentialCitationCount":0,"publicationDate":"10/09/2020","authors":"Junlong Li,Zhuosheng Zhang,Hai Zhao","id":"8089bfe8aa59151147b78d9c9968026119cd5420","summary":"Experimental results on widely used open-domain response selection and quality estimation benchmarks show that DAPO significantly improves the baseline models and achieves state-of-the-art performance on the MuTual leaderboard, verifying the effectiveness of estimating quality evaluation factors into pre-training.","score":1},{"url":"https://www.semanticscholar.org/paper/7f3fb456319181ee092b4e335302fb953523aaba","title":"Towards Emotion-Aware Agents For Negotiation Dialogues","venue":"Affective Computing and Intelligent Interaction","year":2021,"referenceCount":40,"citationCount":4,"influentialCitationCount":2,"publicationDate":"28/07/2021","authors":"Kushal Chawla,Rene Clever,Jaysa Ramirez,Gale M. Lucas,J. Gratch","id":"7f3fb456319181ee092b4e335302fb953523aaba","summary":"This work explores the prediction of two important subjective goals in a negotiation – outcome satisfaction and partner perception and studies three degrees of emotion dimensions – emoticons, lexical, and contextual by leveraging affective lexicons and a state-of-the-art deep learning architecture.","score":1},{"url":"https://www.semanticscholar.org/paper/ec64e324ce1210fe5245dfd0fb5a92058732e5b9","title":"Benchmarking Generalization via In-Context Instructions on 1, 600+ Language Tasks","venue":"ArXiv","year":2022,"referenceCount":61,"citationCount":31,"influentialCitationCount":8,"publicationDate":2022,"authors":"Yizhong Wang,Swaroop Mishra,Pegah Alipoormolabashi,Yeganeh Kordi,Amirreza Mirzaei,Anjana Arunkumar,Arjun Ashok,Arut Selvan Dhanasekaran,Atharva Naik,David Stap,Eshaan Pathak,Giannis Karamanolakis,Haizhi Gary Lai,I. Purohit,Ishani Mondal,Jacob Anderson,Kirby Kuznia,Krima Doshi,Maitreya Patel,Kuntal Kumar Pal,M. Moradshahi,Mihir Parmar,Mirali Purohit,Neeraj Varshney,Phani Rohitha Kaza,Pulkit Verma,Ravsehaj Singh Puri,Rushang Karia,Shailaja Keyur Sampat,Savan Doshi,S. Mishra,Sujan Reddy,Sumanta Patro,Tanay Dixit,Xudong Shen,Chitta Baral,Yejin Choi,Hannaneh Hajishirzi,Noah A. Smith,Daniel Khashabi","id":"ec64e324ce1210fe5245dfd0fb5a92058732e5b9","summary":"This work introduces N ATURAL -I NSTRUCTIONS v 2, a collection of 1,600+ diverse language tasks and their expert written instructions that covers 70+ distinct task types, such as tagging, in-ﬁlling, and rewriting.","score":1},{"url":"https://www.semanticscholar.org/paper/06d7cb8c8816360feb33c3367073e0ef66d7d0b0","title":"Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks","venue":"","year":2022,"referenceCount":51,"citationCount":9,"influentialCitationCount":2,"publicationDate":"16/04/2022","authors":"Yizhong Wang,Swaroop Mishra,Pegah Alipoormolabashi,Yeganeh Kordi,Amirreza Mirzaei,Anjana Arunkumar,Arjun Ashok,Arut Selvan Dhanasekaran,Atharva Naik,David Stap,Eshaan Pathak,Giannis Karamanolakis,Haizhi Gary Lai,I. Purohit,Ishani Mondal,Jacob Anderson,Kirby Kuznia,Krima Doshi,Maitreya Patel,Kuntal Kumar Pal,M. Moradshahi,Mihir Parmar,Mirali Purohit,Neeraj Varshney,Phani Rohitha Kaza,Pulkit Verma,Ravsehaj Singh Puri,Rushang Karia,Shailaja Keyur Sampat,Savan Doshi,S. Mishra,Sujan Reddy,Sumanta Patro,Tanay Dixit,Xudong Shen,Chitta Baral,Yejin Choi,Noah A. Smith,Hanna Hajishirzi,Daniel Khashabi","id":"06d7cb8c8816360feb33c3367073e0ef66d7d0b0","summary":"Tk-Instruct is built, a transformer model trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples) that outperforms existing instruction-following models such as InstructGPT by over 9% on the authors' benchmark despite being an order of magnitude smaller.","score":1},{"url":"https://www.semanticscholar.org/paper/304c860cd33f6630fc2fb0d14b6f3ca62b0fa7dc","title":"Opponent Modeling in Negotiation Dialogues by Related Data Adaptation","venue":"NAACL-HLT","year":2022,"referenceCount":40,"citationCount":3,"influentialCitationCount":1,"publicationDate":"30/04/2022","authors":"Kushal Chawla,Gale M. Lucas,Jonathan May,J. Gratch","id":"304c860cd33f6630fc2fb0d14b6f3ca62b0fa7dc","summary":"This work proposes a ranker for identifying these priorities from negotiation dialogues and devise ways to adapt related data sources for this task to provide more explicit supervision for incorporating the opponent’s preferences and offers, as a proxy to relying on granular utterance-level annotations.","score":1},{"url":"https://www.semanticscholar.org/paper/c44176020bc7034f5ea788cb8de7fcdda5f6a91d","title":"Social Influence Dialogue Systems: A Scoping Survey of the Efforts Towards Influence Capabilities of Dialogue Systems","venue":"ArXiv","year":2022,"referenceCount":93,"citationCount":0,"influentialCitationCount":0,"publicationDate":2022,"authors":"Kushal Chawla,Weiyan Shi,Jingwen Zhang,Gale M. Lucas,Zhou Yu,J. Gratch","id":"c44176020bc7034f5ea788cb8de7fcdda5f6a91d","summary":"This work formally deﬁne and introduces the category of social inﬂuence dialogue systems that in-situ users’ cognitive and emotional responses are changed, leading to changes in thoughts, opinions, and behaviors through natural conversations.","score":1},{"url":"https://www.semanticscholar.org/paper/a1275e92f4830e5bbd53bc1b1fa44a9a2f024f88","title":"Social Influence Dialogue Systems: A Survey of Datasets and Models For Social Influence Tasks","venue":"","year":2022,"referenceCount":98,"citationCount":0,"influentialCitationCount":0,"publicationDate":"11/10/2022","authors":"Kushal Chawla,Weiyan Shi,Jingwen Zhang,Gale M. Lucas,Zhou Yu,J. Gratch","id":"a1275e92f4830e5bbd53bc1b1fa44a9a2f024f88","summary":"This work formally deﬁne and introduces the category of social inﬂuence dialogue systems that in-situ users’ cognitive and emotional responses are changed, leading to changes in thoughts, opinions, and behaviors through natural conversations.","score":1},{"url":"https://www.semanticscholar.org/paper/e89ed6bb1864558e3889f5f2fb8931643c633479","title":"Human-level play in the game of Diplomacy by combining language models with strategic reasoning","venue":"Science","year":2022,"referenceCount":60,"citationCount":15,"influentialCitationCount":1,"publicationDate":"22/11/2022","authors":"A. Bakhtin,Noam Brown,Emily Dinan,Gabriele Farina,Colin Flaherty,Daniel Fried,Andrew Goff,Jonathan Gray,Hengyuan Hu,Athul Paul Jacob,Mojtaba Komeili,Karthik Konath,Minae Kwon,Adam Lerer,Mike Lewis,Alexander H. Miller,S. Mitts,Adithya Renduchintala,Stephen Roller,Dirk Rowe,Weiyan Shi,Joe Spisak,Alexander Wei,David J. Wu,Hugh Zhang,Markus Zijlstra","id":"e89ed6bb1864558e3889f5f2fb8931643c633479","summary":"Cicero, the first AI agent to achieve human-level performance in Diplomacy, a strategy game involving both cooperation and competition that emphasizes natural language negotiation and tactical coordination between seven players, is introduced.","score":1},{"url":"https://www.semanticscholar.org/paper/b4e8ec7547d736a5a006af18d95491f96b294bdc","title":"Werewolf Among Us: A Multimodal Dataset for Modeling Persuasion Behaviors in Social Deduction Games","venue":"ArXiv","year":2022,"referenceCount":32,"citationCount":0,"influentialCitationCount":0,"publicationDate":"16/12/2022","authors":"Bolin Lai,Hongxin Zhang,Miao Liu,Aryan Pariani,Fiona Ryan,Wenqi Jia,Shirley Anugrah Hayati,James M. Rehg,Diyi Yang","id":"b4e8ec7547d736a5a006af18d95491f96b294bdc","summary":"This paper introduces the first multimodal dataset for modeling persuasion behaviors and explores the generalization ability of language models for persuasion modeling and the role of persuasion strategies in predicting social deduction game outcomes.","score":1},{"url":"https://www.semanticscholar.org/paper/0974035826cd6d4be9c604a8679621c8621aff5f","title":"Let's Negotiate! A Survey of Negotiation Dialogue Systems","venue":"ArXiv","year":2022,"referenceCount":57,"citationCount":0,"influentialCitationCount":0,"publicationDate":"18/12/2022","authors":"Haolan Zhan,Yufei Wang,Tao Feng,Yuncheng Hua,Suraj Sharma,Zhuang Li,Lizhen Qu,Gholamreza Haffari","id":"0974035826cd6d4be9c604a8679621c8621aff5f","summary":"This work aims to provide the community with a systematic overview of negotiation dialogue systems, covering benchmarks, evaluations, and methodologies, and to inspire future research.","score":1}]}